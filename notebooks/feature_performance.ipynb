{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e25fce36-e1f2-444a-87a3-98ce51bbd857",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8aff6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "from ml_bets.constants import FEATURES_PATH\n",
    "from ml_bets.features.features import Features\n",
    "from ml_bets.modeling.match_model import PipelineDatasets, run_pycaret_setup\n",
    "from ml_bets.supplementary.functions import create_dataset, add_targets, add_columns_table, train_test_split\n",
    "from feature_selection.feature_selection import FeatureSelection\n",
    "\n",
    "from pycaret.classification import (add_metric,\n",
    "    create_model,\n",
    "    finalize_model,\n",
    "    optimize_threshold,\n",
    "    save_model,\n",
    "    compare_models, \n",
    "    evaluate_model,\n",
    "    get_config,\n",
    "    setup,\n",
    "    tune_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5246bac6-24f5-4778-9ad3-bceb8b867e3d",
   "metadata": {},
   "source": [
    "# Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561e4a75-4d6c-45e7-8a92-cf3001b4bbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_param = {\n",
    "        \"Accuracy\": 0.1,\n",
    "        \"AUC\": 0.1,\n",
    "        \"Recall\": 0.1,\n",
    "        \"Precision\": 0.1,\n",
    "        \"F1\": 0.1,\n",
    "        \"Kappa\": -1.0,\n",
    "        \"MCC\": -1.0,\n",
    "    }  # NEED TO BE FIXED. REWRITE PARAMETERS!!!!! \n",
    "metrics_list = [\"Accuracy\", \"AUC\", \"Recall\", \"Precision\", \"F1\", \"Kappa\", \"MCC\"]\n",
    "user_date = \"20-October-2021\"\n",
    "user_target = 'goals_2.5'\n",
    "cutoff = user_target.split(\"_\")[-1]\n",
    "ignore_features = ['month','date','competition','season_x','month_start_or_end','raw_match_id','season_y']\n",
    "fold_strategy = \"timeseries\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab772006-78cb-49c9-b6fd-efd1ea09b29b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Pycaret setup kwargs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba91d82b-8fd3-40ce-a15e-e1491d70dc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_kwargs = dict(\n",
    "        preprocess=True,\n",
    "        train_size=0.75,\n",
    "        session_id=123,\n",
    "        normalize=True,\n",
    "        transformation=True,\n",
    "        ignore_low_variance=True,\n",
    "        remove_multicollinearity=True,\n",
    "        multicollinearity_threshold=0.75,\n",
    "        n_jobs=-1,\n",
    "        use_gpu=False,\n",
    "        profile=False,\n",
    "        ignore_features=ignore_features,\n",
    "        fold_strategy=fold_strategy,\n",
    "        remove_perfect_collinearity=True,\n",
    "        create_clusters=False,\n",
    "        fold=3,\n",
    "        feature_selection=True,\n",
    "        # you can use this to keep the 95 % most relevant features (fat_sel_threshold)\n",
    "        feature_selection_threshold=0.5,\n",
    "        combine_rare_levels=False,\n",
    "        rare_level_threshold=0.02,\n",
    "        pca=False,\n",
    "        pca_method=\"kernel\",\n",
    "        pca_components=30,\n",
    "        polynomial_features=False,\n",
    "        polynomial_degree=2,\n",
    "        polynomial_threshold=0.01,\n",
    "        trigonometry_features=False,\n",
    "        remove_outliers=True,\n",
    "        outliers_threshold=0.01,\n",
    "        feature_ratio=False,\n",
    "        feature_interaction=False,\n",
    "        # Makes everything slow AF. use to find out possibly interesting features\n",
    "        interaction_threshold=0.01,\n",
    "        fix_imbalance=True,\n",
    "        log_experiment=False,\n",
    "        verbose=False,\n",
    "        silent=True,\n",
    "        experiment_name=\"lagstest\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783f0048-5499-453d-8210-89e227d25b85",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Creation dataset (standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d79ebba-4ac2-4b91-b1f4-d1e496187d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_dataset()\n",
    "df = add_targets(df=df, cutoff=cutoff)\n",
    "# Train-test split\n",
    "train, test = train_test_split(df=df, sep=user_date) # insert table to evaluate and separation date "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446247eb-6347-4bfc-a1fe-6fd1327dd228",
   "metadata": {},
   "source": [
    "## Pycaret evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f31290-da78-4fda-ba17-cac16cd008c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerics = [\"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\", \"int\", \"float\"]\n",
    "num_cols = train.select_dtypes(include=numerics).columns.tolist()\n",
    "setup_kwargs['numeric_features'] = num_cols "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc228fc4-233e-471a-8977-24da60d7bfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = setup(data=train, target=user_target, **setup_kwargs)\n",
    "x_train = get_config('X_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1597070c-847c-4772-83df-16c66fcf144e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_models = compare_models(\n",
    "            n_select=5,\n",
    "            sort='AUC',\n",
    "            exclude=[\"qda\", \"knn\", \"nb\", 'catboost'],\n",
    "            verbose=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca784b5-a863-4bf7-a455-4b29cca4d2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(top_models[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e43057b-18dd-46ac-b7cc-ab3ca2e6d873",
   "metadata": {},
   "source": [
    "## Finalize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24740ad3-5d28-4469-a005-a5db458410f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = finalize_model(top_models[0]) # Choose the preferred model. Best performance model selected by default. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf37cc8-5663-446e-ad6b-e003917f7776",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Metrics against test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a246d7-e05d-4e71-89c7-ed72783ae1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict_model(final, data=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c3c73e-e192-493e-9703-1cbe12f0e556",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict = {\n",
    "    metric: check_metric(\n",
    "        actual=predictions[user_target], \n",
    "        prediction=predictions['Label'], \n",
    "        metric=metric\n",
    "    ) for metric in metrics_list\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e581bb7-d5f6-49db-bf99-406c8b082749",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame(columns=metrics_dict.keys(), index=pd.Index(range(1)))\n",
    "metrics_df.index = ['standard_model']\n",
    "for col, val in metrics_dict.items():\n",
    "    metrics_df.loc['standard_model', col] = val \n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820c8fac-65c3-4f91-9b76-a16bd5d119f8",
   "metadata": {},
   "source": [
    "# Creation dataset (with new columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7dd6c4-66cf-4ad5-9c55-4448cb315bc0",
   "metadata": {},
   "source": [
    "### Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd080dd4-7e75-4f2d-b5d6-2f3e6fab03d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = Features(output=features_path)\n",
    "examples = features.create()\n",
    "examples = examples[~examples[f\"prob_under_goals_{cutoff}\"].isna()]\n",
    "# Initialize PipelineDataset class\n",
    "pipe_ds = PipelineDatasets(\n",
    "    features=features,\n",
    "    target=user_target,\n",
    "    examples=examples,\n",
    "    test_size=user_date,\n",
    ")\n",
    "n_train, n_test = pipe_ds.train_data, pipe_ds.test_data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b289db75",
   "metadata": {},
   "source": [
    "## Referee table. Merging dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5fe53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.getcwd() \n",
    "path = Path(current_path).parent / 'data/features/referee_table.csv'\n",
    "referee_table = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280b2313-d2cc-4509-a860-d8625608459d",
   "metadata": {},
   "outputs": [],
   "source": [
    "referee_table = add_columns_table(feat=features, df=referee_table) # add unique id_match to referee table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495f4e0f-25d4-4c75-a717-64226a146203",
   "metadata": {},
   "outputs": [],
   "source": [
    "referee_cols  = referee_table.columns.tolist() \n",
    "referee_cols.remove('competition')\n",
    "referee_cols.remove('date') # Remove repeated columns \n",
    "referee_train = pd.merge(n_train, referee_table[referee_cols], left_index=True, right_index=True, how='inner') # Referee data + train old data\n",
    "referee_test = pd.merge(n_test, referee_table[referee_cols], left_index=True, right_index=True, how='inner') # Referee data + test old data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104c3762-3b69-4f78-aa72-c8a2c9063550",
   "metadata": {},
   "source": [
    "## Selection of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba2bfcd-1e5b-4b95-8142-0dd81f6084dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_sel = FeatureSelection(target=user_target, user_df=referee_train, target_features=0.4, filter_metrics=metric_param)\n",
    "selected_features = feat_sel.repeat_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce352f45-9d69-4783-8e9b-48a6023c3fe5",
   "metadata": {},
   "source": [
    "## Pycaret evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec995282-c5b6-4406-b443-115202966b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "referee_train = referee_train[selected_features + [user_target]]\n",
    "numerics = [\"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\", \"int\", \"float\"]\n",
    "num_cols = referee_train.select_dtypes(include=numerics).columns.tolist()\n",
    "setup_kwargs['numeric_features'] = num_cols "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffe4915-1cc2-49a0-8a4e-750ab4fe79d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = setup(data=referee_train, target=user_target, **setup_kwargs)\n",
    "x_train = get_config('X_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e34937e-a743-4099-841c-6159fdd06a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_models = compare_models(\n",
    "            n_select=5,\n",
    "            sort='AUC',\n",
    "            exclude=[\"qda\", \"knn\", \"nb\", 'catboost'],\n",
    "            verbose=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9ec301-45eb-4594-83df-c642cf13c592",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(top_models[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a96856a-d9ae-414a-b3bb-1acf8d5eec8c",
   "metadata": {},
   "source": [
    "## Finalize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcd25ac-87b5-4c3d-90b7-f0e3cf1461ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = finalize_model(top_models[0]) # Choose the preferred model. Best performance model selected by default. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12704063-466c-4e26-af72-d877362a051c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Metrics against test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e887740-9aaa-4892-8e8e-362602096c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict_model(final, data=referee_test[selected_features + [user_target]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a42818-26ff-4b90-89dc-d3985c8f4ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict = {\n",
    "    metric: check_metric(\n",
    "        actual=predictions[user_target], \n",
    "        prediction=predictions['Label'], \n",
    "        metric=metric\n",
    "    ) for metric in metrics_list\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd13657e-f540-4625-a98c-8bdeb1bf299b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_metrics_df = pd.DataFrame(columns=metrics_dict.keys(), index=pd.Index(range(1)))\n",
    "new_model_metrics_df.index = ['new_feat_model']\n",
    "for col, val in metrics_dict.items():\n",
    "    new_model_metrics_df.loc['new_feat_model', col] = val \n",
    "new_model_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b493f633-c80a-4e9a-a95d-abe48fc85947",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([metrics_df, new_model_metrics_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8e36e1-a24b-4104-ac1f-a47701cbbbf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
